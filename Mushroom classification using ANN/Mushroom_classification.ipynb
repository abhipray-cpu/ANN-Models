{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mushroom classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TaG0264nmG_",
        "outputId": "3f5b0b1b-9f23-409f-f414-6544ad4f7bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensor-dash in /usr/local/lib/python3.7/dist-packages (1.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tensor-dash) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tensor-dash) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tensor-dash) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tensor-dash) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tensor-dash) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "#importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "!pip install tensor-dash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate your dataset\n",
        "def generate_data(location:str,sample_number=10):\n",
        "  data=pd.read_csv(location,engine='python')\n",
        "  head=data.head()\n",
        "  tail=data.tail()\n",
        "  sample=data.sample(sample_number)\n",
        "  description=data.describe()\n",
        "  columns=data.columns\n",
        "  info=data.info()\n",
        "  shape=data.shape\n",
        "  size=data.size\n",
        "  return_data = {'data':data,'head':head,'tail':tail,'sample':sample,'description':description,'columns':columns,'info':info,\n",
        "          'shape':shape,'size':size}\n",
        "  return return_data"
      ],
      "metadata": {
        "id": "MlhHLA_gnsW_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set=generate_data('/content/mushrooms.csv')\n",
        "data=data_set['data']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2IacYvVnvXu",
        "outputId": "d412f34e-b75a-484d-b0a3-19e4c003d625"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8124 entries, 0 to 8123\n",
            "Data columns (total 23 columns):\n",
            " #   Column                    Non-Null Count  Dtype \n",
            "---  ------                    --------------  ----- \n",
            " 0   class                     8124 non-null   object\n",
            " 1   cap-shape                 8124 non-null   object\n",
            " 2   cap-surface               8124 non-null   object\n",
            " 3   cap-color                 8124 non-null   object\n",
            " 4   bruises                   8124 non-null   object\n",
            " 5   odor                      8124 non-null   object\n",
            " 6   gill-attachment           8124 non-null   object\n",
            " 7   gill-spacing              8124 non-null   object\n",
            " 8   gill-size                 8124 non-null   object\n",
            " 9   gill-color                8124 non-null   object\n",
            " 10  stalk-shape               8124 non-null   object\n",
            " 11  stalk-root                8124 non-null   object\n",
            " 12  stalk-surface-above-ring  8124 non-null   object\n",
            " 13  stalk-surface-below-ring  8124 non-null   object\n",
            " 14  stalk-color-above-ring    8124 non-null   object\n",
            " 15  stalk-color-below-ring    8124 non-null   object\n",
            " 16  veil-type                 8124 non-null   object\n",
            " 17  veil-color                8124 non-null   object\n",
            " 18  ring-number               8124 non-null   object\n",
            " 19  ring-type                 8124 non-null   object\n",
            " 20  spore-print-color         8124 non-null   object\n",
            " 21  population                8124 non-null   object\n",
            " 22  habitat                   8124 non-null   object\n",
            "dtypes: object(23)\n",
            "memory usage: 1.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_type(data):\n",
        "  numeric=[]\n",
        "  categorical=[]\n",
        "  for col in data.columns:\n",
        "    if data[f'{col}'].dtypes == 'object':\n",
        "      categorical.append(col)\n",
        "    else:\n",
        "      numeric.append(col)\n",
        "  return {'numeric':numeric,'categorical':categorical}"
      ],
      "metadata": {
        "id": "o1ZVgbtNn8M9"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for null values and deal with them\n",
        "# this function will take the type of process as well for both numeric and categorical data\n",
        "def treat_null_values(data,numeric_type:str='mean'):\n",
        "  types=get_type(data)\n",
        "  numeric=types['numeric']\n",
        "  categorical=types['categorical']\n",
        "  if numeric_type == 'mean':\n",
        "    for col in numeric:\n",
        "      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].mean())\n",
        "  elif numeric_type == 'mode':\n",
        "    for col in numeric:\n",
        "      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].mode())\n",
        "  elif numeric_type == 'median':\n",
        "    for col in numeric:\n",
        "      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].median())\n",
        "  elif numeric_type == 'frequent':\n",
        "    for col in numeric:\n",
        "      data[f'{col}']=data[f'{col}'].fillna(data[f'{col}'].nunique[0])\n",
        "  elif numeric_type == 'drop':\n",
        "    for col in numeric:\n",
        "      data[f'{col}']=data[f'{col}'].dropnna(inplace=True)\n",
        "  elif numeric_type == 'predictive_modeling':\n",
        "    pass # create a seprate function for this\n",
        "  elif numeric_type == 'impute':\n",
        "    pass # create a seprate function for this as well\n",
        "  \n",
        "  for col in categorical:\n",
        "    most_frequent_category=data[f'{col}'].mode()[0]\n",
        "    data[f'{col}'].fillna(most_frequent_category,inplace=True)\n",
        "  return data\n",
        "\n",
        "  \n",
        "\n",
        "def predictive_modeling():\n",
        "  pass #do a detailed study as disadvantages for this model usually outweights advantages\n",
        "def multiple_imputation():\n",
        "  from fancyimpute import IterativeImputer as MICE\n",
        "  data= pd.DataFrame(MICE().fit_transform(data))\n",
        "  return data"
      ],
      "metadata": {
        "id": "_TtrLSbFn-9L"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_data(data,multiclass:str='One_hot',binary_class:str='Label'): #this function takes three args one is the data 2nd is the type of encoding for multiclass data and third is the encoding for binary class data\n",
        "  categorical=get_type(data)['categorical']\n",
        "  multivariate=[]\n",
        "  bivariate=[]\n",
        "  for col in categorical:\n",
        "    if data[f'{col}'].nunique()>2:\n",
        "      multivariate.append(col)\n",
        "    else:\n",
        "      bivariate.append(col)\n",
        "  \n",
        "  if multiclass == 'One_hot':\n",
        "    for col in multivariate:\n",
        "      data=encode_and_bind(data,col)\n",
        "  if binary_class == 'Label':\n",
        "    for col in bivariate:\n",
        "      data=label_encode(data,col)\n",
        "  # add other sorting techniques as well in here\n",
        "  return data\n",
        "\n",
        "def encode_and_bind(original_dataframe, feature_to_encode):\n",
        "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
        "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
        "    res.pop(feature_to_encode)\n",
        "    return(res)\n",
        "\n",
        "def label_encode(data,col):\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "  encoder=LabelEncoder()\n",
        "  data[col]=encoder.fit_transform(data[col])\n",
        "  return data\n",
        "  \n",
        "# this function needs to be modified therefore add differenr sort of encoding techniques as well in this cll"
      ],
      "metadata": {
        "id": "I7Cn2tOIoBvj"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treat_outliers(data,feature:str,algo:str='IQR',z_threshold:int=3,add_feature=None,remove:bool=False,scatter:bool=False,feature_x=None,feature_y=None): # this function takes two arguments one is data and 2ns is the algorithm\n",
        "  pass\n",
        "  if scatter == True:\n",
        "    px.scatter(data_px,x=feature_x,y=feature_y,hover_name='Country')\n",
        "  else:\n",
        "    if algo == 'IQR':\n",
        "      outlier_index=Inter_quantile_range(data,feature)\n",
        "    if algo == 'EEA': # elliptic envelope algo\n",
        "      outlier_index=Elliptice_envelope_algo(data,feature,add_feature)\n",
        "    if algo == 'ISF': # Isolate forest algo\n",
        "      outlier_index=Isolate_forest_algo(data,feature)\n",
        "    if algo == 'One_classSVM': # one class svm\n",
        "      outlier_index=One_class_SVM(data,feature)\n",
        "    if algo == 'LOF':\n",
        "      outlier_index=Local_factor_outlier(data,feature)\n",
        "    if algo == 'Z_score':\n",
        "      outlier_index=Z_score_algo(data,feature,z_threshold)\n",
        "    print(outlier_index)\n",
        "    if remove == True:\n",
        "      data=remove_outliers(outlier_index,data)\n",
        "    \n",
        "    return data\n",
        "    \n",
        "\n",
        "\n",
        "# in all these function display the index of outliers\n",
        "\n",
        "def Local_factor_outlier(data,feature):\n",
        "  import numpy as np \n",
        "  from sklearn.neighbors import LocalOutlierFactor\n",
        "  X=data[[feature,data.columns.values[-1]]]\n",
        "  lof = LocalOutlierFactor(n_neighbors=20, algorithm='auto',\n",
        "                         metric='minkowski', contamination=0.04,\n",
        "                         novelty=False, n_jobs=-1)\n",
        "  pred = lof.fit_predict(X)\n",
        "  outlier_index = np.where(pred==-1)\n",
        "  return outlier_index\n",
        "\n",
        "def One_class_SVM(data,feature): #this returns novelty I am guessing so fuck this method\n",
        "  import numpy as np\n",
        "  from sklearn.svm import OneClassSVM\n",
        "  X=data[[feature,data.columns.values[-1]]]\n",
        "  one_class_svm = OneClassSVM(kernel='rbf', degree=3, gamma='scale')\n",
        "  new_data = np.array([[-4, 8.5]])# change these values as per your dataset\n",
        "  one_class_svm.fit(X)\n",
        "  pred = one_class_svm.predict(new_data)\n",
        "  outlier_index = np.where(pred==-1)\n",
        "  return outlier_index\n",
        "\n",
        "def Isolate_forest_algo(data,feature):\n",
        "  import numpy as np\n",
        "  from sklearn.ensemble import IsolationForest\n",
        "  from sklearn.decomposition import PCA\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  X=data[[feature,data.columns.values[-1]]]\n",
        "   # Returns 1 of inliers, -1 for outliers\n",
        "  iforest = IsolationForest(n_estimators=100, max_samples='auto', \n",
        "                          contamination=0.05, max_features=1.0, \n",
        "                          bootstrap=False, n_jobs=-1, random_state=1)\n",
        "  pred = iforest.fit_predict(X)\n",
        " # Extract outliers\n",
        "  outlier_index = np.where(pred==-1)\n",
        "  return outlier_index\n",
        "\n",
        "def Elliptice_envelope_algo(data,feature,add_feature):\n",
        "  import numpy as np\n",
        "  from sklearn.covariance import EllipticEnvelope\n",
        "  elpenv = EllipticEnvelope(contamination=0.025, \n",
        "                          random_state=1)\n",
        "  X=data[[feature,add_feature]]\n",
        "# Returns 1 of inliers, -1 for outliers\n",
        "  pred = elpenv.fit_predict(X)\n",
        "\n",
        "# Extract outliers\n",
        "  outlier_index = np.where(pred==-1)\n",
        "  return outlier_index\n",
        "\n",
        "def Inter_quantile_range(data,feature):\n",
        "  Q1 = np.percentile(data[f'{feature}'], 25, interpolation = 'midpoint') \n",
        "  Q2 = np.percentile(data[f'{feature}'], 50, interpolation = 'midpoint') \n",
        "  Q3 = np.percentile(data[f'{feature}'], 75, interpolation = 'midpoint') \n",
        "  IQR = Q3 - Q1 \n",
        "  low_lim = Q1 - 1.5 * IQR\n",
        "  up_lim = Q3 + 1.5 * IQR\n",
        "  outlier_index=[]\n",
        "  for val in data[f'{feature}']:\n",
        "    if val > up_lim or val <low_lim:\n",
        "      outlier_index.append(data.index[data[f'{feature}'] == val].values[0])\n",
        "  return outlier_index\n",
        "         \n",
        "def Z_score_algo(data,feature,z_threshold):\n",
        "  mean = np.mean(data[f'{feature}'])\n",
        "  std = np.std(data[f'{feature}'])\n",
        "  print('mean of the dataset is', mean)\n",
        "  print('std. deviation is', std)\n",
        "  outlier_index = []\n",
        "  for val in data[f'{feature}']:\n",
        "    z = (val-mean)/std\n",
        "    if z > z_threshold:\n",
        "      outlier_index.append(data.index[data[f'{feature}'] == val].values[0])\n",
        "  return outlier_index\n",
        "  \n",
        "\n",
        "\n",
        "# this function will remove the outliers if removal is allowed\n",
        "def remove_outliers(index,data):\n",
        "  for row in index:\n",
        "    data.drop([row])\n",
        "    print(f'dropped value at index {row}')\n",
        "  return data\n",
        "\n",
        "      \n",
        "\n",
        "# if remove is true remove outliers else if it is false just return the index of outliers\n",
        "# this will function will be returning two datasets the original one and one in which all the outliers are removed\n",
        "\n"
      ],
      "metadata": {
        "id": "RIlRta6NoD3K"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function will perform all the data preprocessing steps\n",
        "# incase you want to use non default algos call these function seprately\n",
        "def preprocess_data(data,feature:str):\n",
        "  data=treat_null_values(data)\n",
        "  data=encode_data(data)\n",
        "  last_column = data[feature]\n",
        "  data.drop(feature, inplace=True, axis=1)\n",
        "  data.insert(data.shape[1],feature,last_column)\n",
        "  \n",
        "  return data\n",
        "\n",
        "# call outliers and balance function independentlu whenever you like and PS Fuck Roopa\n"
      ],
      "metadata": {
        "id": "_93CaesMoF45"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can further preprocess the data you want in this code cell like removing some values and handle some corner cases\n",
        "#problem arising here is that the last column is getting modified as well into one hot encoding \n",
        "#modifying the funciton\n",
        "data=preprocess_data(data,'class')"
      ],
      "metadata": {
        "id": "_fd6eI-WoKOD"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "Lq3HvDzEoKZT",
        "outputId": "0c872a69-4126-4c14-d10c-854097af922e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      bruises  gill-attachment  gill-spacing  gill-size  stalk-shape  \\\n",
              "0           1                1             0          1            0   \n",
              "1           1                1             0          0            0   \n",
              "2           1                1             0          0            0   \n",
              "3           1                1             0          1            0   \n",
              "4           0                1             1          0            1   \n",
              "...       ...              ...           ...        ...          ...   \n",
              "8119        0                0             0          0            0   \n",
              "8120        0                0             0          0            0   \n",
              "8121        0                0             0          0            0   \n",
              "8122        0                1             0          1            1   \n",
              "8123        0                0             0          0            0   \n",
              "\n",
              "      veil-type  cap-shape_b  cap-shape_c  cap-shape_f  cap-shape_k  ...  \\\n",
              "0             0            0            0            0            0  ...   \n",
              "1             0            0            0            0            0  ...   \n",
              "2             0            1            0            0            0  ...   \n",
              "3             0            0            0            0            0  ...   \n",
              "4             0            0            0            0            0  ...   \n",
              "...         ...          ...          ...          ...          ...  ...   \n",
              "8119          0            0            0            0            1  ...   \n",
              "8120          0            0            0            0            0  ...   \n",
              "8121          0            0            0            1            0  ...   \n",
              "8122          0            0            0            0            1  ...   \n",
              "8123          0            0            0            0            0  ...   \n",
              "\n",
              "      population_v  population_y  habitat_d  habitat_g  habitat_l  habitat_m  \\\n",
              "0                0             0          0          0          0          0   \n",
              "1                0             0          0          1          0          0   \n",
              "2                0             0          0          0          0          1   \n",
              "3                0             0          0          0          0          0   \n",
              "4                0             0          0          1          0          0   \n",
              "...            ...           ...        ...        ...        ...        ...   \n",
              "8119             0             0          0          0          1          0   \n",
              "8120             1             0          0          0          1          0   \n",
              "8121             0             0          0          0          1          0   \n",
              "8122             1             0          0          0          1          0   \n",
              "8123             0             0          0          0          1          0   \n",
              "\n",
              "      habitat_p  habitat_u  habitat_w  class  \n",
              "0             0          1          0      1  \n",
              "1             0          0          0      0  \n",
              "2             0          0          0      0  \n",
              "3             0          1          0      1  \n",
              "4             0          0          0      0  \n",
              "...         ...        ...        ...    ...  \n",
              "8119          0          0          0      0  \n",
              "8120          0          0          0      0  \n",
              "8121          0          0          0      0  \n",
              "8122          0          0          0      1  \n",
              "8123          0          0          0      0  \n",
              "\n",
              "[8124 rows x 113 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-111610d7-fdc2-4700-a51a-8d3b9e667deb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bruises</th>\n",
              "      <th>gill-attachment</th>\n",
              "      <th>gill-spacing</th>\n",
              "      <th>gill-size</th>\n",
              "      <th>stalk-shape</th>\n",
              "      <th>veil-type</th>\n",
              "      <th>cap-shape_b</th>\n",
              "      <th>cap-shape_c</th>\n",
              "      <th>cap-shape_f</th>\n",
              "      <th>cap-shape_k</th>\n",
              "      <th>...</th>\n",
              "      <th>population_v</th>\n",
              "      <th>population_y</th>\n",
              "      <th>habitat_d</th>\n",
              "      <th>habitat_g</th>\n",
              "      <th>habitat_l</th>\n",
              "      <th>habitat_m</th>\n",
              "      <th>habitat_p</th>\n",
              "      <th>habitat_u</th>\n",
              "      <th>habitat_w</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8119</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8120</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8121</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8122</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8123</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8124 rows Ã— 113 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-111610d7-fdc2-4700-a51a-8d3b9e667deb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-111610d7-fdc2-4700-a51a-8d3b9e667deb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-111610d7-fdc2-4700-a51a-8d3b9e667deb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_and_scale(x,y,scale:str=\"standardize\",split_precent:int=0.2):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=split_precent,random_state=42)\n",
        "  if scale == 'standardize':\n",
        "    res=standardize(x_train,x_test)\n",
        "    return {'x_train':res['x_train'],'x_test':res['x_test'],'y_train':y_train,'y_test':y_test,'scaler':res['scaler']}\n",
        "  else:\n",
        "    res=normalize(x_train,x_test)\n",
        "    return {'x_train':res['x_train'],'x_test':res['x_test'],'y_train':y_train,'y_test':y_test,'scaler':res['scaler']}\n",
        "\n",
        "def standardize(x_train,x_test):\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  st=StandardScaler()\n",
        "  x_train=st.fit_transform(x_train)\n",
        "  x_test=st.transform(x_test)\n",
        "  return {'x_train':x_train,'x_test':x_test,'scaler':st}\n",
        "\n",
        "\n",
        "def normalize(x_train,x_test):\n",
        "  from sklearn.preprocessing import normalize\n",
        "  nm=normalize()\n",
        "  x_train=nm.fit_transform(x_train)\n",
        "  x_test=nm.transform(x_test)\n",
        "  return {'x_train':x_train,'x_test':x_test,'scaler':nm}\n"
      ],
      "metadata": {
        "id": "108yDXTsoKb1"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=data.iloc[:,:-1].values #these are the independent variables\n",
        "y=data.iloc[:,-1].values #these are the dependent variables"
      ],
      "metadata": {
        "id": "KxQgGUL9oGDz"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_Data=split_and_scale(x,y)"
      ],
      "metadata": {
        "id": "sUdfIzuhoGGj"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scalerObj=final_Data['scaler']\n",
        "x_train=final_Data['x_train']\n",
        "x_test=final_Data['x_test']\n",
        "y_train=final_Data['y_train']\n",
        "y_test=final_Data['y_test']"
      ],
      "metadata": {
        "id": "3y2xfH8LoGL4"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensor-dash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YtICCAnoGQz",
        "outputId": "fdda6a85-4a0a-4402-8096-92ef31a7e0d8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensor-dash in /usr/local/lib/python3.7/dist-packages (1.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tensor-dash) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tensor-dash) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tensor-dash) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tensor-dash) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tensor-dash) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#modify this function to take kwargs and then use the key value pair\n",
        "def create_initializeModel(layers:list): #this list will contain three parametres number of layers,neurons in each layer,and the activation function\n",
        "  #initializing the ANN\n",
        "  ann=tf.keras.models.Sequential()\n",
        "  #adding the layers\n",
        "  for layer in layers:\n",
        "    ann.add(tf.keras.layers.Dense(units=layer['neurons'],activation=layer['activation']))\n",
        "  return ann\n",
        "\n",
        "#since this is a multiclass classification therfore the activation funciton in the output layer will be softmax\n",
        "ann=create_initializeModel([{\"neurons\":6,'activation':'relu'},{\"neurons\":6,'activation':'relu'},{\"neurons\":1,'activation':'sigmoid'}])\n",
        "#need to upgrade this function for cetain options"
      ],
      "metadata": {
        "id": "rCl85jMVoGTj"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#need to update this function for certain options\n",
        "def compileModel(model,optimizer:str='adam',loss:str='binary_crossentropy',metrics:list=['accuracy']):\n",
        "  ann.compile(optimizer=optimizer,loss=loss,metrics=metrics)\n",
        "  return ann\n",
        " \n",
        "ann=compileModel(ann,loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "PRUR9rc8obS8"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensordash.tensordash import Tensordash\n",
        "histories=Tensordash(\n",
        "    ModelName='ann Clasification (mushroom classification model)',\n",
        "    email='dumkaabhipray@gmail.com',\n",
        "    password='kamalanita1@')\n",
        "def trainModel(ann,x_train,y_train,batchSize:int=32,epochs:int=100):\n",
        "  try:\n",
        "    ann.fit(x_train,y_train,batch_size=batchSize,epochs=epochs,callbacks=[histories])\n",
        "    return ann\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    histories.sendCrash()\n",
        "\n",
        "ann=trainModel(ann,x_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJVx_XhzobVt",
        "outputId": "fd28f068-bdd3-4e62-f06c-218b70b62eb5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "204/204 [==============================] - 2s 5ms/step - loss: 0.4271 - accuracy: 0.8344\n",
            "Epoch 2/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 0.1039 - accuracy: 0.9849\n",
            "Epoch 3/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 0.0374 - accuracy: 0.9966\n",
            "Epoch 4/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 0.0134 - accuracy: 0.9998\n",
            "Epoch 5/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 8.7285e-04 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 7.5076e-04 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 6.5869e-04 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 5.8504e-04 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 5.2642e-04 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 4.7674e-04 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "204/204 [==============================] - 2s 8ms/step - loss: 4.3387e-04 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 3.9708e-04 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 3.6476e-04 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 3.3734e-04 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 3.0162e-04 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 2.8168e-04 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.6432e-04 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.4785e-04 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.3364e-04 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.2074e-04 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.0925e-04 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.9930e-04 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 1.8983e-04 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.8060e-04 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "204/204 [==============================] - 2s 8ms/step - loss: 1.7190e-04 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.6346e-04 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.5491e-04 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 1.4705e-04 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.3902e-04 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.3136e-04 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "204/204 [==============================] - 2s 10ms/step - loss: 1.2387e-04 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 1.1669e-04 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.0997e-04 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 1.0316e-04 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 9.6687e-05 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 9.0129e-05 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 8.4291e-05 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 7.8519e-05 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 7.3215e-05 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 6.8210e-05 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 6.3432e-05 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 5.8734e-05 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 5.4551e-05 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 5.0427e-05 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 4.6766e-05 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 4.3177e-05 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 4.0018e-05 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 3.6752e-05 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 3.3910e-05 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 3.1259e-05 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 2.8709e-05 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 2.6437e-05 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.4292e-05 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.2427e-05 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 2.0688e-05 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.8908e-05 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.7350e-05 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.5930e-05 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.4612e-05 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 1.3465e-05 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.2310e-05 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 1.1282e-05 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.0381e-05 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 9.5005e-06 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "204/204 [==============================] - 2s 8ms/step - loss: 8.6987e-06 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 7.9672e-06 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 7.3184e-06 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 6.6809e-06 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 6.1185e-06 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 5.5918e-06 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 5.1298e-06 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 4.6854e-06 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "204/204 [==============================] - 2s 8ms/step - loss: 4.3347e-06 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 3.9228e-06 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 3.5949e-06 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 3.2874e-06 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 3.0256e-06 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.7612e-06 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.5257e-06 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.3097e-06 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 2.1099e-06 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "204/204 [==============================] - 2s 8ms/step - loss: 1.9300e-06 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.7703e-06 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.6177e-06 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.4808e-06 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 1.3536e-06 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.2335e-06 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.1334e-06 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 1.0318e-06 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 9.4229e-07 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 8.6278e-07 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 7.9183e-07 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 7.2255e-07 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "204/204 [==============================] - 2s 9ms/step - loss: 6.6176e-07 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predictResults(ann,x_test,y_train):\n",
        "  #predicting test set results\n",
        "  y_pred=ann.predict(x_test)#this will simply return probablity\n",
        "  y_pred=(y_pred>0.5)#we need to convert probablity to some solid value based on some threshold value\n",
        "  res=np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)\n",
        "  print(res)\n",
        "  ax1=sns.distplot(y_test,hist=False,color=\"r\",label=\"actual value\")\n",
        "  sns.distplot(y_pred,hist=False,color=\"b\",label=\"predicted values\",ax=ax1)\n",
        "predictResults(ann,x_test,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "i9PE8FhBobYc",
        "outputId": "614f3f12-7d3a-4b78-d374-e85d40ae5ab5"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0]\n",
            " [1 1]\n",
            " [1 1]\n",
            " ...\n",
            " [1 1]\n",
            " [1 1]\n",
            " [1 1]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `kdeplot` (an axes-level function for kernel density plots).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def crossValidateModel(ann,x_train,y_train,batchSize:int=32,epoch:int=100):\n",
        "  accuracies=[]\n",
        "  for val in range(1,10):\n",
        "    print(f'this is the {val}th dataset split results')\n",
        "    histories_validate=Tensordash(\n",
        "    ModelName=f'ann Clasification (mushroom classification) {val}th dataset split',\n",
        "    email='dumkaabhipray@gmail.com',\n",
        "    password='kamalanita1@')\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=val+7)\n",
        "    try:\n",
        "      history=ann.fit(np.asarray(x_train).astype(np.float32), np.asarray(y_train).astype(np.float32),batch_size=batchSize,epochs=epoch,callbacks=[histories_validate])\n",
        "      accuracies.append(history)\n",
        "    except Exception as e:\n",
        "      print(f'model crashed for {val}th iteration')\n",
        "      print(e)\n",
        "      histories_validate.sendCrash()\n",
        "  return accuracies\n",
        "\n",
        "accuracies=crossValidateModel(ann,x_train,y_train)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io7FUbl5oba6",
        "outputId": "2575cc5c-390a-454f-f8b7-a865895134d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is the 1th dataset split results\n",
            "Epoch 1/100\n",
            "191/191 [==============================] - 2s 13ms/step - loss: 5.9919e-05 - accuracy: 1.0000\n",
            "Epoch 2/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 4.8530e-05 - accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 3.8393e-05 - accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 3.0343e-05 - accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 2.5173e-05 - accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 2.1311e-05 - accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 1.8474e-05 - accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 1.5966e-05 - accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 1.3929e-05 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 1.2135e-05 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 1.0707e-05 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 9.3774e-06 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 8.2361e-06 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 7.2698e-06 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 6.3992e-06 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 5.6679e-06 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 5.0162e-06 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 4.4529e-06 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 3.9557e-06 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 3.5093e-06 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 3.1397e-06 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 2.7836e-06 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 2.4900e-06 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 2.2140e-06 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 1.9748e-06 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 1.7698e-06 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 1.5749e-06 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "191/191 [==============================] - 1s 8ms/step - loss: 1.4154e-06 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 1.2600e-06 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 1.1146e-06 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 9.9072e-07 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "191/191 [==============================] - 2s 12ms/step - loss: 8.7611e-07 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 7.8640e-07 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "191/191 [==============================] - 2s 11ms/step - loss: 6.9986e-07 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 6.2570e-07 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "191/191 [==============================] - 2s 11ms/step - loss: 5.5871e-07 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "191/191 [==============================] - 2s 8ms/step - loss: 4.9877e-07 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "191/191 [==============================] - 2s 12ms/step - loss: 4.4630e-07 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 3.9940e-07 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "191/191 [==============================] - 2s 12ms/step - loss: 3.5758e-07 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "191/191 [==============================] - 2s 11ms/step - loss: 3.2202e-07 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "191/191 [==============================] - 2s 12ms/step - loss: 2.8885e-07 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 2.5936e-07 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 2.3295e-07 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 2.0995e-07 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 1.8922e-07 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 1.7185e-07 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "191/191 [==============================] - 2s 11ms/step - loss: 1.5440e-07 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "191/191 [==============================] - 2s 12ms/step - loss: 1.4004e-07 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 1.2613e-07 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 1.1404e-07 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 1.0302e-07 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 9.3254e-08 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 8.4884e-08 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "191/191 [==============================] - 2s 12ms/step - loss: 7.6351e-08 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 6.9282e-08 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 6.3249e-08 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 5.7524e-08 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 5.2210e-08 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 4.7293e-08 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 4.3194e-08 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 3.9327e-08 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 3.5752e-08 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "191/191 [==============================] - 2s 9ms/step - loss: 3.2662e-08 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 2.9843e-08 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 2.7265e-08 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 2.4918e-08 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 2.2719e-08 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 2.0788e-08 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 1.9100e-08 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 1.7628e-08 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 1.6077e-08 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 1.4804e-08 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 1.3613e-08 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 1.2574e-08 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 1.1629e-08 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 1.0751e-08 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 9.9490e-09 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 9.1903e-09 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 8.5160e-09 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 7.9402e-09 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 7.3376e-09 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 6.8196e-09 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 6.3522e-09 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 5.9244e-09 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 5.5382e-09 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 5.1584e-09 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 4.8181e-09 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 4.5108e-09 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "191/191 [==============================] - 1s 7ms/step - loss: 4.2344e-09 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 3.9798e-09 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 3.7343e-09 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 3.4903e-09 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 3.3039e-09 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 3.1114e-09 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 2.9392e-09 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 2.7955e-09 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 2.6226e-09 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 2.4937e-09 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "191/191 [==============================] - 2s 10ms/step - loss: 2.3672e-09 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display the results in  here\n",
        "#modify this function to be dynamic\n",
        "def createDataFrames():\n",
        "  accuracy_Data=[]\n",
        "  loss_Data=[]\n",
        "  for history in accuracies:\n",
        "    accuracy_arr=[]\n",
        "    accuracy_arr.append(sum(history.history['accuracy']) / len(history.history['accuracy']))\n",
        "    loss_arr=[]\n",
        "    loss_arr.append(sum(history.history['loss']) / len(history.history['loss']))\n",
        "    for accuracy_val, loss_val in zip(history.history['accuracy'],history.history['loss']):\n",
        "      accuracy_arr.append(accuracy_val)\n",
        "      loss_arr.append(loss_val)\n",
        "    \n",
        "    accuracy_Data.append(accuracy_arr)\n",
        "    loss_Data.append(loss_arr)\n",
        "  #creating dataframes from these values\n",
        "  columns=['mean']\n",
        "  for i in range(1,101):\n",
        "    columns.append(f'Epoch{i}')\n",
        "  index=[]\n",
        "  for i in range(1,len(accuracy_Data)+1):\n",
        "    index.append(f'Iteration{i}')\n",
        "  print(columns)\n",
        "  print(index)\n",
        "  #need to handle the case for failing iterations by simply making this function dynamic \n",
        "  #this is the dataframe for accuracies\n",
        "  accuracy_DF = pd.DataFrame(data=accuracy_Data,index=index,columns=columns)\n",
        "  accuracy_DF = accuracy_DF.sort_values(by=['mean'],ascending=False)\n",
        "  #this is the dataframe for losses\n",
        "  loss_DF = pd.DataFrame(data=loss_Data,index=index,columns=columns)\n",
        "  loss_DF = loss_DF.sort_values(by=['mean'],ascending=False)\n",
        "\n",
        "  return {'accuracies':accuracy_DF,'losses':loss_DF}\n",
        "\n",
        "dfs=createDataFrames()"
      ],
      "metadata": {
        "id": "q2Ik-vo5obdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the accuracy dataframe\n",
        "display(dfs['accuracies'].sort_values(by=['mean'],ascending=False))"
      ],
      "metadata": {
        "id": "PQJxk_Ayobiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the loss dataframe\n",
        "display(dfs['losses'].sort_values(by=['mean'],ascending=False))"
      ],
      "metadata": {
        "id": "MIa_d0plopfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'This is the mean accuracy of the model after applying 10 cross validations {dfs[\"accuracies\"][\"mean\"].mean()}')"
      ],
      "metadata": {
        "id": "Q0KTsh10opie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'This is the mean loss of the model after applying 10 cross validations {dfs[\"losses\"][\"mean\"].mean()}')"
      ],
      "metadata": {
        "id": "SwVSZSJOopoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#crating 2 lists one for binary categrorical and second for more then two class categorical\n",
        "binary=[]\n",
        "oneHot=[]\n",
        "def findCategorical(data):\n",
        "  types=get_type(data)\n",
        "  catg=types['categorical']\n",
        "  for col in catg:\n",
        "    if data[f\"{col}\"].nunique() == 2 :\n",
        "      binary.append(col)\n",
        "      #since in this case we are skipping first three fields (3) and later the row number will not be included therfore -1 =2\n",
        "      #can take this as well as input parametre\n",
        "    elif data[f'{col}'].nunique() > 2:\n",
        "      oneHot.append(col)\n",
        "\n",
        "data=pd.read_csv('/content/Churn_Modelling.csv')\n",
        "#since the iniial data has no correlation with the dependent variable\n",
        "data=data.iloc[:,3:-1]\n",
        "findCategorical(data)\n",
        "print(f'This is the list of fields which needs to be binary encoded: {binary}')\n",
        "print(f'This is the list of fields which needs to be one Hot encoded: {oneHot}')"
      ],
      "metadata": {
        "id": "5WLR4AxJoprC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this funnction will encode binary categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "binaryDictionary={}\n",
        "oneHotDictionary={}\n",
        "def encodeBinary():\n",
        "  le=LabelEncoder()\n",
        "  for field in binary:\n",
        "    values=np.unique(data[f'{field}'].values)\n",
        "    encodes=le.fit_transform(np.unique(data[f'{field}'].values))\n",
        "    bin={}\n",
        "    for val,encode in zip(values,encodes):\n",
        "      bin[val]=encode\n",
        "    binaryDictionary[f'{field}']=bin\n",
        "    \n",
        "    \n",
        "#this function will encode one Hot categorical\n",
        "def encodeOneHot():\n",
        "  for field in oneHot:\n",
        "    values=np.unique(data[f'{field}'].values)\n",
        "    dummies = pd.get_dummies(np.unique(data[[f'{field}']].values))\n",
        "    dummies=dummies.values\n",
        "    oneDic={}\n",
        "    for val,encode in zip(values,dummies):\n",
        "      oneDic[f'{val}']=encode\n",
        "    oneHotDictionary[f'{field}']=oneDic\n",
        "\n",
        "encodeBinary()\n",
        "encodeOneHot()"
      ],
      "metadata": {
        "id": "UesKLOnJopwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this function takes in a number of inputs maps the categorical ones and then predict the results from the tuned and trained model\n",
        "#pass in a list of named feature to this function\n",
        "#since will be passing named parametres therfore will first match the bin dict\n",
        "#and then will match one Hot dict\n",
        "#for one hot dict need to add len(array) values on place of one value\n",
        "#for this maintain an array and keep on appending values which will eventually create\n",
        "#a dataframe entry\n",
        "def predictor(features:dict):\n",
        "  inputFeatres=[] \n",
        "  for key, value in list(features.items())[2:]:#removing the first two non correlated features\n",
        "    #checking for binary categorical\n",
        "    if key in binaryDictionary:\n",
        "      inputFeatres.append(binaryDictionary[key][value])\n",
        "    #checking for oneHot categorical\n",
        "    elif key in oneHotDictionary:\n",
        "      dummies=oneHotDictionary[key][value]\n",
        "      for el in dummies:\n",
        "        inputFeatres.append(el)\n",
        "    #else will be numeric feature\n",
        "    else:\n",
        "      inputFeatres.append(value)\n",
        "  #scaling the features array using the scaler object we used earlier\n",
        "  inputFeatres=scalerObj.transform([inputFeatres])\n",
        "  #this part will needs to be modified depending on the problem statement\n",
        "  result=ann.predict(inputFeatres)\n",
        "  if result==False:\n",
        "    print(f'{features[\"mushroom\"]} is not poisonous')\n",
        "  else:\n",
        "    print(f'{features[\"Surname\"]} is poisonous')\n",
        "\n",
        "#modify this parametre list accordingly\n",
        "predictor({\n",
        "    'CustomerId':'696969',\n",
        "    'Surname':'Puttanpal',\n",
        "    'CreditScore':600,\n",
        "    'Geography':'France',\n",
        "    'Gender':'Male',\n",
        "    'Age':40,\n",
        "    'Tenure':3,\n",
        "    'Balance':60000,\n",
        "    'NumOfProducts':2,\n",
        "    'HasCrCard':1,\n",
        "    'IsActiveMember':1,\n",
        "    'EstimatedSalary':5000,\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "id": "q7CjHezGoxsz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}